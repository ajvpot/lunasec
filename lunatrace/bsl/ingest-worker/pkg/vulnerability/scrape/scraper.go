package scrape

import (
	"context"
	"crypto/tls"
	"database/sql"
	"encoding/json"
	"net/http"
	"net/url"
	"os"
	"path"
	"strings"
	"sync"
	"time"

	"github.com/PullRequestInc/go-gpt3"
	"github.com/go-jet/jet/v2/postgres"
	"github.com/go-shiori/go-readability"
	"github.com/mozillazg/go-slugify"
	"github.com/rs/zerolog/log"
	"github.com/schollz/progressbar/v3"
	"go.uber.org/fx"
	"gorm.io/gorm"

	"github.com/lunasec-io/lunasec/lunatrace/bsl/ingest-worker/pkg/pineconefx"
	packschem "github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/package/table"
	"github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/vulnerability/model"
	"github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/vulnerability/table"
)

var Module = fx.Options(
	fx.Provide(
		NewConfig,
		NewScraper,
	),
)

type Scraper interface {
	ScrapeVulnerabilities(cache, vulnID string) error
	GenerateEmbeddingsForContent(cache, vulnID string) error
	LoadAndOutputToDir(cache string, outputDir string, markdown bool) error
}

type scraperDeps struct {
	fx.In
	Config

	DB             *sql.DB
	OpenAIClient   gpt3.Client
	PineconeClient pineconefx.PineconeClient
}

type scraper struct {
	deps           scraperDeps
	httpClient     *http.Client
	browserDomains []string
}

func (p *scraper) scrapeVulnerabilityReference(sourceID string, ref model.Reference) *ProcessedReference {
	pRef := ProcessedReference{
		VulnerabilityID: sourceID,
		URL:             ref.URL,
		SuccessfulFetch: false,
	}

	content, contentType, err := p.scrapeContent(ref.URL)
	if err != nil {
		log.Error().Err(err).Str("url", ref.URL).Msg("failed to scrape content")
		return &pRef
	}

	pRef.SuccessfulFetch = true
	pRef.Content = content

	normalContent, err := normalizeReferenceContent(content)
	if err != nil {
		log.Error().
			Err(err).
			Str("url", ref.URL).
			Msg("failed to normalize content")
		return &pRef
	}

	pRef.NormalizedContent = normalContent
	pRef.ContentType = contentType

	return &pRef
}

func (p *scraper) scrapeVulnerability(
	vulnInfo VulnerabilityInfo,
) *ProcessedVulnerability {
	processedVuln := ProcessedVulnerability{
		ID: vulnInfo.SourceID,
	}

	if vulnInfo.Details != nil {
		processedVuln.Details = *vulnInfo.Details
	}
	if vulnInfo.Summary != nil {
		processedVuln.Summary = *vulnInfo.Summary
	}

	for _, ref := range vulnInfo.References {
		pRef := p.scrapeVulnerabilityReference(vulnInfo.SourceID, ref)
		processedVuln.References = append(processedVuln.References, pRef)
	}
	return &processedVuln
}

func (p *scraper) processVulnerabilityWorker(
	wg *sync.WaitGroup,
	vulnInfoChan <-chan *VulnerabilityInfo,
	saveRefChan chan<- *ProcessedReference,
) error {
	for vulnInfo := range vulnInfoChan {
		processed := p.scrapeVulnerability(*vulnInfo)
		for _, ref := range processed.References {
			saveRefChan <- ref
		}
	}
	wg.Done()
	return nil
}

func (p *scraper) updateOrCreateWorker(
	db *gorm.DB,
	saveWg *sync.WaitGroup,
	saveRefChan <-chan *ProcessedReference,
) {
	saveWg.Add(1)
	defer saveWg.Done()

	for ref := range saveRefChan {
		var processedRef ProcessedReference
		res := db.Where(&ProcessedReference{
			URL: ref.URL,
		}).First(&processedRef)
		if res.Error == nil {
			processedRef.Content = ref.Content
			processedRef.SuccessfulFetch = ref.SuccessfulFetch
			processedRef.Title = ref.Title
			processedRef.NormalizedContent = ref.NormalizedContent
			processedRef.VulnerabilityID = ref.VulnerabilityID
			db.Save(&processedRef)
			continue
		}

		res = db.Create(&ref)
		if res.Error != nil {
			log.Error().
				Err(res.Error).
				Str("url", ref.URL).
				Msg("failed to insert reference")
			continue
		}
	}
}

func (p *scraper) ScrapeVulnerabilities(cache, vulnID string) error {
	db, err := loadGormDB(cache)
	if err != nil {
		return err
	}

	countStmt := table.Vulnerability.SELECT(
		postgres.COUNT(table.Vulnerability.ID),
	)

	var count struct {
		model.Vulnerability
		Count int `json:"count"`
	}

	err = countStmt.Query(p.deps.DB, &count)
	if err != nil {
		return err
	}

	whereClause := packschem.Package.PackageManager.EQ(postgres.NewEnumValue("npm"))
	if vulnID != "" {
		whereClause = whereClause.AND(table.Vulnerability.SourceID.EQ(postgres.String(vulnID)))
	}

	s := table.Vulnerability.SELECT(
		table.Vulnerability.ID,
		table.Vulnerability.SourceID,
		table.Vulnerability.Details,
		table.Vulnerability.Summary,
		table.Reference.URL,
	).FROM(
		table.Vulnerability.INNER_JOIN(
			table.Reference, table.Reference.VulnerabilityID.EQ(table.Vulnerability.ID),
		).INNER_JOIN(
			table.Affected, table.Affected.VulnerabilityID.EQ(table.Vulnerability.ID),
		).INNER_JOIN(
			packschem.Package, packschem.Package.ID.EQ(table.Affected.PackageID),
		),
	).WHERE(whereClause).ORDER_BY(table.Vulnerability.SourceID.DESC())

	rows, err := s.Rows(context.Background(), p.deps.DB)
	if err != nil {
		log.Error().Err(err).Msg("failed to get vulnerability rows")
		return err
	}
	defer rows.Close()

	log.Info().Int("count", count.Count).Msg("processing vulnerabilities for npm")

	bar := progressbar.Default(int64(count.Count))

	vulnInfoChan := make(chan *VulnerabilityInfo, 100)
	saveRefChan := make(chan *ProcessedReference, 100)

	var (
		wg     sync.WaitGroup
		saveWg sync.WaitGroup
	)

	for i := 0; i < p.deps.Workers; i++ {
		wg.Add(1)
		go func() {
			err := p.processVulnerabilityWorker(&wg, vulnInfoChan, saveRefChan)
			if err != nil {
				log.Error().Err(err).Msg("failed to process vulnerability worker")
			}
		}()
	}

	go p.updateOrCreateWorker(db, &saveWg, saveRefChan)

	for rows.Next() {
		bar.Add(1)

		var vulnInfo struct {
			VulnerabilityInfo
			Reference string `json:"reference"`
		}
		err = rows.Rows.Scan(&vulnInfo.ID, &vulnInfo.SourceID, &vulnInfo.Details, &vulnInfo.Summary, &vulnInfo.Reference)
		if err != nil {
			log.Error().Err(err).Msg("failed to get vulnerability row")
			return err
		}

		vulnURL := "https://lunatrace.lunasec.io/vulnerabilities/" + vulnInfo.SourceID

		existingVulnRef := ProcessedReference{
			URL: vulnURL,
		}

		res := db.Where(&existingVulnRef)
		if res.Error != nil {
			var title string
			var content string
			if vulnInfo.Summary != nil {
				title = *vulnInfo.Summary
			}
			if vulnInfo.Details != nil {
				content = *vulnInfo.Details
			}

			vulnRef := ProcessedReference{
				VulnerabilityID: vulnInfo.SourceID,
				URL:             vulnURL,
				Title:           title,
				Content:         content,
			}
			saveRefChan <- &vulnRef
		}

		// add the reference to the vulnerability, and add the vulnerability to the channel.
		// the channel will be processed by a worker which expects an array of references, but we are currently
		// only processing one reference at a time. (see sql query above)
		vulnInfo.References = append(vulnInfo.References, model.Reference{
			URL: vulnInfo.Reference,
		})
		vulnInfoChan <- &vulnInfo.VulnerabilityInfo
	}

	close(vulnInfoChan)
	log.Info().Msg("waiting for workers to finish")
	wg.Wait()

	close(saveRefChan)
	log.Info().Msg("waiting for references to finish saving")
	saveWg.Wait()

	return nil
}

func (p *scraper) LoadAndOutputToDir(cache string, outputDir string, markdown bool) error {
	db, err := loadGormDB(cache)
	if err != nil {
		return err
	}

	err = os.MkdirAll(outputDir, 0755)
	if err != nil {
		return err
	}

	rows, err := db.Table("processed_references").Rows()
	if err != nil {
		return err
	}
	defer rows.Close()

	for rows.Next() {
		var ref ProcessedReference
		err = db.ScanRows(rows, &ref)
		if err != nil {
			log.Error().Err(err).Msg("failed to scan reference")
			continue
		}

		var (
			content []byte
			ext     string
		)

		contentReader := strings.NewReader(ref.Content)

		parsedUrl, err := url.Parse(ref.URL)
		if err != nil {
			continue
		}

		article, err := readability.FromReader(contentReader, parsedUrl)
		if err != nil {
			log.Error().Err(err).Msg("failed to parse html body")
			continue
		}

		if markdown {
			ext = ".md"

			strContent, err := formatContentAsMarkdown(ref.Content, ref.URL)
			if err != nil {
				log.Warn().Err(err).Msg("failed to convert reference to markdown")
				content = []byte("# " + ref.Title + "\n\n" + "## Vulnerability" + "\n[[" + ref.VulnerabilityID + "]]\n\n" + article.TextContent)
			} else {
				content = []byte(strContent)
			}
		} else {
			ext = ".json"

			ref.Title = article.Title
			ref.Content = article.TextContent

			content, err = json.Marshal(ref)
			if err != nil {
				log.Error().Err(err).Msg("failed to serialize reference")
				continue
			}
		}

		err = os.WriteFile(path.Join(outputDir, slugify.Slugify(ref.URL)+ext), content, 0644)
		if err != nil {
			log.Error().Err(err).Msg("failed to write reference")
			continue
		}
	}
	return nil
}

func NewScraper(deps scraperDeps) Scraper {
	tr := &http.Transport{
		TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
	}

	client := &http.Client{Timeout: time.Second * 5, Transport: tr}

	parsedBrowserDomains := strings.Split(strings.ReplaceAll(deps.BrowserDomains, " ", ""), ",")

	return &scraper{
		deps:           deps,
		httpClient:     client,
		browserDomains: parsedBrowserDomains,
	}
}
