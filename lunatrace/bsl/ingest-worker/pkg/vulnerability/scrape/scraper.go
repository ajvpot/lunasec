package scrape

import (
	"context"
	"crypto/tls"
	"database/sql"
	"encoding/json"
	"net/http"
	"net/url"
	"os"
	"path"
	"strings"
	"sync"
	"time"

	"github.com/PullRequestInc/go-gpt3"
	"github.com/go-jet/jet/v2/postgres"
	"github.com/go-shiori/go-readability"
	"github.com/google/uuid"
	"github.com/mozillazg/go-slugify"
	"github.com/rs/zerolog/log"
	"github.com/schollz/progressbar/v3"
	"go.uber.org/fx"

	"github.com/lunasec-io/lunasec/lunatrace/bsl/ingest-worker/pkg/pineconefx"
	"github.com/lunasec-io/lunasec/lunatrace/bsl/ingest-worker/pkg/util"
	packschem "github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/package/table"
	"github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/vulnerability/model"
	"github.com/lunasec-io/lunasec/lunatrace/gogen/sqlgen/lunatrace/vulnerability/table"
)

var Module = fx.Options(
	fx.Provide(
		NewConfig,
		NewScraper,
	),
)

type Scraper interface {
	ScrapeVulnerabilities(vulnID string) error
	GenerateEmbeddingsForContent(vulnID string, insertWithPinecone bool) error
	LoadAndOutputToDir(cache string, outputDir string, markdown bool) error
	ScrapeURLWithChrome(url string) (*ScrapeResponse, error)
	SearchForReferences(search string) (string, error)
}

type scraperDeps struct {
	fx.In
	Config

	DB             *sql.DB
	OpenAIClient   gpt3.Client
	PineconeClient pineconefx.PineconeClient
}

type scraper struct {
	deps           scraperDeps
	httpClient     *http.Client
	browserDomains []string
}

func (p *scraper) scrapeVulnerabilityReference(ref *model.Reference) *model.ReferenceContent {
	pRef := model.ReferenceContent{
		ReferenceID: ref.ID,
	}

	resp, err := p.scrapeContent(ref.URL)
	if err != nil {
		log.Error().Err(err).Str("url", ref.URL).Msg("failed to scrape Content")
		return &pRef
	}

	pRef.LastSuccessfulFetch = util.Ptr(time.Now())
	pRef.Content = resp.Content
	pRef.Title = resp.Title

	// TODO break this out into its own step so that normalization is done completely after scraping
	normalContent, err := normalizeReferenceContent(resp.Content)
	if err != nil {
		log.Error().
			Err(err).
			Str("url", ref.URL).
			Msg("failed to normalize Content")
		return &pRef
	}

	pRef.NormalizedContent = normalContent
	pRef.ContentType = resp.ContentType

	return &pRef
}

func (p *scraper) processVulnerabilityWorker(
	wg *sync.WaitGroup,
	refScrapeChan <-chan *model.Reference,
	saveRefChan chan<- *model.ReferenceContent,
) error {
	for ref := range refScrapeChan {
		scrapedRef := p.scrapeVulnerabilityReference(ref)
		saveRefChan <- scrapedRef
	}
	wg.Done()
	return nil
}

func (p *scraper) referenceContentAlreadyExists(referenceID string) error {
	referenceUUID, err := uuid.Parse(referenceID)
	if err != nil {
		return err
	}

	rc := table.ReferenceContent
	selectExistingRef := rc.LEFT_JOIN(
		table.Reference, table.Reference.ID.EQ(rc.ReferenceID),
	).SELECT(
		table.Reference.ID,
	).WHERE(
		table.Reference.ID.EQ(postgres.UUID(referenceUUID)),
	)

	var existingVulnRef model.ReferenceContent
	return selectExistingRef.Query(p.deps.DB, &existingVulnRef)
}

func (p *scraper) updateOrCreateRefContentWorker(
	saveWg *sync.WaitGroup,
	saveRefChan <-chan *model.ReferenceContent,
) {
	saveWg.Add(1)
	defer saveWg.Done()

	for ref := range saveRefChan {
		rc := table.ReferenceContent
		upsertRefContent := rc.INSERT(
			rc.ReferenceID,
			rc.ContentType,
			rc.Content,
			rc.NormalizedContent,
			rc.Title,
			rc.LastSuccessfulFetch,
		).MODEL(ref).ON_CONFLICT(
			rc.ReferenceID,
		).DO_UPDATE(
			postgres.SET(
				rc.ContentType.SET(
					rc.EXCLUDED.ContentType,
				),
				rc.Content.SET(
					rc.EXCLUDED.Content,
				),
				rc.NormalizedContent.SET(
					rc.EXCLUDED.NormalizedContent,
				),
				rc.Title.SET(
					rc.EXCLUDED.Title,
				),
				rc.LastSuccessfulFetch.SET(
					rc.EXCLUDED.LastSuccessfulFetch,
				),
			),
		)

		_, err := upsertRefContent.Exec(p.deps.DB)
		if err != nil {
			log.Error().
				Err(err).
				Msg("failed to upsert reference Content")
			continue
		}
	}
}

func (p *scraper) ScrapeVulnerabilities(vulnID string) error {
	countStmt := table.Vulnerability.SELECT(
		postgres.COUNT(table.Vulnerability.ID),
	)

	var count struct {
		model.Vulnerability
		Count int `json:"count"`
	}

	err := countStmt.Query(p.deps.DB, &count)
	if err != nil {
		return err
	}

	whereClause := packschem.Package.PackageManager.EQ(postgres.NewEnumValue("npm"))
	if vulnID != "" {
		whereClause = whereClause.AND(table.Vulnerability.SourceID.EQ(postgres.String(vulnID)))
	}

	s := table.Vulnerability.SELECT(
		table.Vulnerability.ID,
		table.Vulnerability.SourceID,
		table.Vulnerability.Details,
		table.Vulnerability.Summary,
		table.Reference.ID,
		table.Reference.URL,
	).FROM(
		table.Vulnerability.INNER_JOIN(
			table.Reference, table.Reference.VulnerabilityID.EQ(table.Vulnerability.ID),
		).INNER_JOIN(
			table.Affected, table.Affected.VulnerabilityID.EQ(table.Vulnerability.ID),
		).INNER_JOIN(
			packschem.Package, packschem.Package.ID.EQ(table.Affected.PackageID),
		),
	).WHERE(whereClause).ORDER_BY(table.Vulnerability.SourceID.DESC())

	rows, err := s.Rows(context.Background(), p.deps.DB)
	if err != nil {
		log.Error().Err(err).Msg("failed to get vulnerability rows")
		return err
	}
	defer rows.Close()

	log.Info().Int("count", count.Count).Msg("processing vulnerabilities for npm")

	bar := progressbar.Default(int64(count.Count))

	refScrapeChan := make(chan *model.Reference, 100)
	saveRefChan := make(chan *model.ReferenceContent, 100)

	var (
		wg     sync.WaitGroup
		saveWg sync.WaitGroup
	)

	for i := 0; i < p.deps.Workers; i++ {
		wg.Add(1)
		go func() {
			err := p.processVulnerabilityWorker(&wg, refScrapeChan, saveRefChan)
			if err != nil {
				log.Error().Err(err).Msg("failed to process vulnerability worker")
			}
		}()
	}

	go p.updateOrCreateRefContentWorker(&saveWg, saveRefChan)

	for rows.Next() {
		bar.Add(1)

		var vulnInfo struct {
			VulnerabilityInfo
			ReferenceID  uuid.UUID
			ReferenceURL string
		}
		err = rows.Rows.Scan(&vulnInfo.ID, &vulnInfo.SourceID, &vulnInfo.Details, &vulnInfo.Summary, &vulnInfo.ReferenceID, &vulnInfo.ReferenceURL)
		if err != nil {
			log.Error().Err(err).Msg("failed to get vulnerability row")
			return err
		}

		err = p.referenceContentAlreadyExists(vulnInfo.ReferenceID.String())
		if err == nil {
			var (
				title   string
				content string
			)

			if vulnInfo.Summary != nil {
				title = *vulnInfo.Summary
			}
			if vulnInfo.Details != nil {
				content = *vulnInfo.Details
			}

			vulnRef := model.ReferenceContent{
				ReferenceID: vulnInfo.ReferenceID,
				Title:       title,
				Content:     content,
			}
			saveRefChan <- &vulnRef
		}

		// TODO check the error to make sure it is just a "not found" error
		// "operator does not exist: uuid = text at character"

		// send this reference to be scraped
		refScrapeChan <- &model.Reference{
			ID:  vulnInfo.ReferenceID,
			URL: vulnInfo.ReferenceURL,
		}
	}

	close(refScrapeChan)
	log.Info().Msg("waiting for workers to finish")
	wg.Wait()

	close(saveRefChan)
	log.Info().Msg("waiting for references to finish saving")
	saveWg.Wait()

	return nil
}

func (p *scraper) LoadAndOutputToDir(cache string, outputDir string, markdown bool) error {
	db, err := loadGormDB(cache)
	if err != nil {
		return err
	}

	err = os.MkdirAll(outputDir, 0755)
	if err != nil {
		return err
	}

	rows, err := db.Table("processed_references").Rows()
	if err != nil {
		return err
	}
	defer rows.Close()

	for rows.Next() {
		var ref ProcessedReference
		err = db.ScanRows(rows, &ref)
		if err != nil {
			log.Error().Err(err).Msg("failed to scan reference")
			continue
		}

		var (
			content []byte
			ext     string
		)

		contentReader := strings.NewReader(ref.Content)

		parsedUrl, err := url.Parse(ref.URL)
		if err != nil {
			continue
		}

		article, err := readability.FromReader(contentReader, parsedUrl)
		if err != nil {
			log.Error().Err(err).Msg("failed to parse html body")
			continue
		}

		if markdown {
			ext = ".md"

			strContent, err := formatContentAsMarkdown(ref.Content, ref.URL)
			if err != nil {
				log.Warn().Err(err).Msg("failed to convert reference to markdown")
				content = []byte("# " + ref.Title + "\n\n" + "## Vulnerability" + "\n[[" + ref.VulnerabilityID + "]]\n\n" + article.TextContent)
			} else {
				content = []byte(strContent)
			}
		} else {
			ext = ".json"

			ref.Title = article.Title
			ref.Content = article.TextContent

			content, err = json.Marshal(ref)
			if err != nil {
				log.Error().Err(err).Msg("failed to serialize reference")
				continue
			}
		}

		err = os.WriteFile(path.Join(outputDir, slugify.Slugify(ref.URL)+ext), content, 0644)
		if err != nil {
			log.Error().Err(err).Msg("failed to write reference")
			continue
		}
	}
	return nil
}

func NewScraper(deps scraperDeps) Scraper {
	tr := &http.Transport{
		TLSClientConfig: &tls.Config{InsecureSkipVerify: true},
	}

	client := &http.Client{Timeout: time.Second * 5, Transport: tr}

	parsedBrowserDomains := strings.Split(strings.ReplaceAll(deps.BrowserDomains, " ", ""), ",")

	return &scraper{
		deps:           deps,
		httpClient:     client,
		browserDomains: parsedBrowserDomains,
	}
}
