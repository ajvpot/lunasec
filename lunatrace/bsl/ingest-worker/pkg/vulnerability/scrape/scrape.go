package scrape

import (
	"context"
	"errors"
	"io"
	"net/http"
	"strings"
	"time"

	"github.com/chromedp/cdproto/dom"
	"github.com/chromedp/cdproto/network"
	"github.com/chromedp/chromedp"
	"github.com/rs/zerolog/log"
	"github.com/samber/lo"
)

func (p *scraper) scrapeContent(url string) (string, string, error) {
	if strings.HasSuffix(url, ".pdf") {
		return "", "", errors.New("PDFs are not supported")
	}

	// if there are no browser domains set or the url contains one of the browser domains, scrape with chrome
	shouldScrapeWithBrowser := len(p.browserDomains) == 0 || lo.ContainsBy(p.browserDomains, func(domain string) bool {
		return strings.Contains(url, domain)
	})
	if shouldScrapeWithBrowser {
		return p.scrapeURLWithChrome(url)
	}
	return p.scrapeContentWithHTTPClient(url)
}

func (p *scraper) scrapeContentWithHTTPClient(url string) (string, string, error) {
	req, err := http.NewRequest(http.MethodGet, url, nil)
	if err != nil {
		log.Error().
			Err(err).
			Str("url", url).
			Msg("Failed to create request for reference URL")
		return "", "", err
	}
	req.Header.Set("User-Agent", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36")

	resp, err := p.httpClient.Do(req)
	if err != nil {
		log.Error().
			Err(err).
			Str("url", url).
			Msg("Failed to fetch reference URL")
		return "", "", err
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return "", "", err
	}
	return string(respBody), resp.Header.Get("Content-Type"), nil
}

func listenForNetworkEvent(ctx context.Context) {
	chromedp.ListenTarget(ctx, func(ev interface{}) {
		switch ev := ev.(type) {

		case *network.EventResponseReceived:
			resp := ev.Response
			if len(resp.Headers) != 0 {
				log.Printf("received headers: %s", resp.Headers)
			}
		}
	})
}

func scrapeURLTasks(url string, html *string) chromedp.ActionFunc {
	return func(ctx context.Context) error {
		resp, err := chromedp.RunResponse(ctx, chromedp.Navigate(url))
		if err != nil {
			return err
		}
		if resp.Status != 200 {
			return err
		}

		node, err := dom.GetDocument().Do(ctx)
		if err != nil {
			return err
		}
		*html, err = dom.GetOuterHTML().WithNodeID(node.NodeID).Do(ctx)
		return err
	}
}

func (p *scraper) scrapeURLWithChrome(url string) (string, string, error) {
	ctx, cancel := chromedp.NewContext(
		context.Background(),
		chromedp.WithLogf(log.Printf),
	)
	defer cancel()

	ctx, cancel = context.WithTimeout(ctx, 5*time.Second)
	defer cancel()

	start := time.Now()

	var res string
	err := chromedp.Run(ctx,
		scrapeURLTasks(url, &res),
	)
	if err != nil {
		log.Error().Err(err).Msg("Failed to scrape URL with Chrome")
		return "", "", err
	}

	log.Debug().
		Float64("duration", time.Since(start).Seconds()).
		Msg("Scraped URL with Chrome")
	return res, "text/html", nil
}
